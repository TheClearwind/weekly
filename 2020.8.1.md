#  Self-supervised learning

A large amount of data is not required to run the network well. Our training model uses tags that are part of the input data, not external tags. In self supervised learning, the task used for pretraining is called "pretext task", which helps the model understand the data. Our target mission is called  “*downstream tasks*” 

One of the most important questions to be answered when using self supervised learning in computer vision is: what pretext task should you use. The selected task must be like this. If it is solved, you need to understand your data, which is also needed to solve your downstream task. Once you've trained your model with the pretext task, you can then fine tune it. At this point, you should think of it as a transfer learning problem



一些例子

 https://arxiv.org/abs/1603.08511 

 https://arxiv.org/abs/1603.06668 

 https://arxiv.org/abs/1806.09594 

 https://arxiv.org/abs/1604.07379 

Google UDA -> 一致性损失

