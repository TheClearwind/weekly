## Some thoughts on BN

Recently, I found such a problem while training DSP network: When I tried to add BN to the network there was a gradient explosion. BN has been proved to play a role in stabilizing the training process in a large number of experiments, but why do gradient explosions occur?

1. the influence of batch size

   BN is based on Mini-batch to calculate mean and variance, rather than the whole Training dataset.

   Therefore, when the batch size is too small, the mean value and variance calculated by BN will not be representative. However, due to the limitation of GPU memory, only a smaller bz can be used here.

2. Is BN fit for the task ?

   A large number of experiments show that BN performs well in the classification problem, the fundamental reason is that the classification problem is not sensitive to the scale of the image. But for DSP tasks, image scale is important information. So BN is not suitable for this kind of task. In addition, in tasks such as denoising or super-resolution, most of the bz are relatively small due to hardware limitations, so IN is more suitable.



