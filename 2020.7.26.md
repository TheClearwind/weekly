# AE & VAE

The automatic encoder is the training model which uses the deep neural network as the encoder and decoder to minimize the reconstruction error. In this case, the more complex the network structure is, the more dimension reduction can be made by the self encoder, while the reconfiguration loss is low. Intuitively, if our encoder and decoder have enough degrees of freedom, we can reduce any initial dimension to 1. However, due to no additional constraints, the coding in the hidden space lacks regularity. In other words, any sample in the hidden space may be meaningless after decoding.

Therefore, in order to be able to use our self encoder decoder for generation purposes, we must ensure that the hidden space is sufficiently regular. One possible way to obtain this regularity is to introduce explicit regularization into the training process. In order to introduce some regularization of the hidden space, we modify the encoding decoding process: instead of encoding the input as a single point in the hidden space, we encode it as a probability distribution in the hidden space. By choosing the normal distribution as the coding distribution, we can train the encoder to return the mean and covariance matrix describing the Gaussian distribution. The reason for encoding the input as a distribution with a certain variance rather than a single point is that it can naturally express the regularization of the hidden space: the distribution returned by the encoder is forced to approach the standard normal distribution

This is VAE